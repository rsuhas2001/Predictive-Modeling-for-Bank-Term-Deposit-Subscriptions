{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Checking GPU Availability in Google Colab**\n",
        "\n",
        "Before running heavy computations, it's essential to verify whether a **GPU** is available for acceleration. The following code checks:\n",
        "\n",
        "- If a **GPU is available** (`True` or `False`).\n",
        "- The **name of the GPU** (if one is detected).\n",
        "\n"
      ],
      "metadata": {
        "id": "7DNOExcbKKCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Gk606uD-Fzw2",
        "outputId": "f1e4ed2d-ea2e-47fc-b3b3-53dac2f032b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UnpBgsthHB3Q",
        "outputId": "fcfe8b9f-eb95-405e-c60f-b6fc666cecd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "M_rZCFL6HLBZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Uploading and Preparing the Dataset in Google Colab**\n",
        "\n",
        "Since Google Colab does not provide direct access to local files, we first need to **manually upload the dataset**. Once uploaded, the dataset is loaded into a Pandas DataFrame.\n",
        "\n",
        "### **📌 Steps:**\n",
        "1. **Upload the dataset** manually using Google Colab’s file upload feature.\n",
        "2. **Load the dataset** into a Pandas DataFrame for processing.\n",
        "3. **Separate Features and Target Variable:**\n",
        "   - Independent features (`X_features`) are extracted.\n",
        "   - The target variable (`default_encoded`) is assigned to `y_target`.\n",
        "4. **Split the dataset into training and testing sets**:\n",
        "   - An **80-20 split** ensures a balanced train-test distribution.\n",
        "   - **Stratification** is applied to maintain the same class ratio in both sets.\n",
        "\n",
        "### **📌 Why is This Important?**\n",
        "- **Ensures proper data loading in Colab**, where local file access is limited.\n",
        "- **Prepares the dataset for model training** by creating separate feature and target variables.\n",
        "- **Maintains class balance in training and test sets**, preventing biased model evaluation.\n",
        "\n",
        "### **💡 Next Steps**\n",
        "- **Verify the dataset structure** by displaying the first few rows.\n",
        "- **Check for missing values** before proceeding to model training.\n"
      ],
      "metadata": {
        "id": "-eyC9C-uKgT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload File Manually in Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]  # Get uploaded file name\n",
        "print(f\"File '{filename}' uploaded successfully!\")\n",
        "\n",
        "# Load the CSV into a DataFrame\n",
        "X = pd.read_csv(filename)\n",
        "\n",
        "# Separate Features and Target Variable\n",
        "X_features = X.drop(columns=[\"default_encoded\"])  # Features\n",
        "y_target = X[\"default_encoded\"]  # Target variable\n",
        "\n",
        "# Split into Train-Test (80-20 Split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y_target, test_size=0.2, stratify=y_target, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "SD9qlQErHOSB",
        "outputId": "2c0b58c3-5d97-4845-edf4-d7fa1bb3dcb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-41b1800c-f731-4938-b002-c0fd573cefb6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-41b1800c-f731-4938-b002-c0fd573cefb6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_processing.csv to data_processing.csv\n",
            "File 'data_processing.csv' uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Model 1: Basic Logistic Regression (Class Weights)**\n",
        "### **📌 Sampling Strategy:**  \n",
        "- **No data is removed** from the dataset.  \n",
        "- Uses **`class_weight=\"balanced\"`** to **handle class imbalance dynamically** without modifying the dataset.  \n",
        "\n",
        "### **📌 Preprocessing:**  \n",
        "- **StandardScaler()** is applied to normalize the dataset for better model stability.  \n",
        "\n",
        "### **📌 GPU Usage:**  \n",
        "- **No GPU is used** for this model.  \n",
        "- Logistic Regression runs on **CPU only**, as `sklearn` does not support GPU acceleration.  \n",
        "\n",
        "### **📌 Purpose:**  \n",
        "- Handles **class imbalance** effectively without complex sampling strategies.  \n",
        "- Keeps the approach simple for **quick execution and evaluation**.  \n",
        "- Evaluated using key metrics like **AUC-PR, Recall, Precision, F1-Score, and MCC**.  \n",
        "\n",
        "### **💡 Next Steps:**  \n",
        "- Compare this model with **Random Forest and XGBoost** to analyze performance differences.  \n",
        "- Apply **hyperparameter tuning later** to improve performance.  \n"
      ],
      "metadata": {
        "id": "wJAi5HVsLSnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, classification_report\n",
        "\n",
        "# Standardize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression (Basic Version)\n",
        "log_reg = LogisticRegression(class_weight=\"balanced\", solver=\"saga\", max_iter=1000, random_state=42, n_jobs=-1)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute Metrics\n",
        "log_reg_metrics = {\n",
        "    \"AUC-PR\": average_precision_score(y_test, y_prob),\n",
        "    \"Recall\": recall_score(y_test, y_pred),\n",
        "    \"Precision\": precision_score(y_test, y_pred),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred),\n",
        "    \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
        "}\n",
        "\n",
        "# Display Metrics\n",
        "metrics_df = pd.DataFrame([log_reg_metrics])\n",
        "\n",
        "# Print Summary\n",
        "print(\"\\n🔹 Basic Logistic Regression Results:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Um1mmZIwSwV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fa5b58be-29c0-4b10-ee18-b02a45fd7f27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Basic Logistic Regression Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.71      0.83      8880\n",
            "           1       0.05      0.85      0.10       163\n",
            "\n",
            "    accuracy                           0.71      9043\n",
            "   macro avg       0.52      0.78      0.46      9043\n",
            "weighted avg       0.98      0.71      0.82      9043\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Model 2: Basic Random Forest (SMOTE Oversampling)**\n",
        "\n",
        "### **📌 Sampling Strategy:**  \n",
        "- **SMOTE (Synthetic Minority Oversampling Technique)** is applied to balance the dataset.  \n",
        "- SMOTE **creates synthetic examples** for the minority class (`default_encoded = 1`) to prevent bias toward the majority class (`default_encoded = 0`).  \n",
        "\n",
        "### **📌 Preprocessing & Model Training:**  \n",
        "- **No hyperparameter tuning** is performed to keep the model simple and fast.  \n",
        "- A **basic Random Forest model** is trained with **100 trees (`n_estimators=100`)**.  \n",
        "\n",
        "### **📌 GPU Usage:**  \n",
        "- **No GPU is used** for this model.  \n",
        "- Random Forest runs on **CPU only** in `sklearn`, as it does not support GPU acceleration.  \n",
        "\n",
        "### **📌 Purpose:**  \n",
        "- Uses **SMOTE to balance class distribution**, ensuring fair learning from both classes.  \n",
        "- Focuses on **fast execution** rather than extensive optimization.  \n",
        "- Evaluated using key metrics such as **AUC-PR, Recall, Precision, F1-Score, and MCC**.  \n",
        "\n",
        "### **💡 Next Steps**  \n",
        "- Compare this **basic Random Forest model** with **Logistic Regression (Class Weights)** and **XGBoost (Class Weights, GPU)**.  \n",
        "- If needed, apply **hyperparameter tuning later** for better performance.  \n"
      ],
      "metadata": {
        "id": "UWvbY833NBlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, classification_report\n",
        "\n",
        "# Apply SMOTE for Class Balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train Basic Random Forest Model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Metrics\n",
        "rf_metrics = {\n",
        "    \"AUC-PR\": average_precision_score(y_test, y_prob_rf),\n",
        "    \"Recall\": recall_score(y_test, y_pred_rf),\n",
        "    \"Precision\": precision_score(y_test, y_pred_rf),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_rf),\n",
        "    \"MCC\": matthews_corrcoef(y_test, y_pred_rf)\n",
        "}\n",
        "\n",
        "# Display Metrics\n",
        "metrics_df_rf = pd.DataFrame([rf_metrics])\n",
        "\n",
        "# Print Summary\n",
        "print(\"\\n🔹 Basic Random Forest (SMOTE Oversampling) Results:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qLyGYPG5HmIq",
        "outputId": "078c43da-1bfa-4cb3-ac52-4097863910a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Basic Random Forest (SMOTE Oversampling) Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98      8880\n",
            "           1       0.16      0.20      0.18       163\n",
            "\n",
            "    accuracy                           0.97      9043\n",
            "   macro avg       0.57      0.59      0.58      9043\n",
            "weighted avg       0.97      0.97      0.97      9043\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Model 3: Basic XGBoost (Class Weights, GPU)**\n",
        "\n",
        "### **📌 Sampling Strategy:**  \n",
        "- **No SMOTE or undersampling is used**—instead, XGBoost handles class imbalance using **`scale_pos_weight`**.  \n",
        "- The **`scale_pos_weight`** is calculated as:  \n",
        "  \\[\n",
        "  \\frac{\\text{Number of Non-Defaulters}}{\\text{Number of Defaulters}}\n",
        "  \\]  \n",
        "  This ensures that the model **assigns higher weight** to the minority class (`default_encoded = 1`).  \n",
        "\n",
        "### **📌 Preprocessing & Model Training:**  \n",
        "- **No hyperparameter tuning** is applied to keep the model simple and efficient.  \n",
        "- A **basic XGBoost model** is trained with **GPU acceleration enabled (`device=\"cuda\"`)**.  \n",
        "\n",
        "### **📌 GPU Usage:**  \n",
        "- ✅ **Yes, this model uses GPU for acceleration**.  \n",
        "- The following settings ensure **full GPU utilization**:  \n",
        "  - **`device=\"cuda\"`** → Runs on GPU instead of CPU.  \n",
        "  - **`tree_method=\"hist\"`** → Optimized method for fast GPU-based training.  \n",
        "- **Faster execution compared to CPU-based training**.  \n",
        "\n",
        "### **📌 Purpose:**  \n",
        "- Uses **class weights (`scale_pos_weight`) instead of data resampling**, keeping all data intact.  \n",
        "- Focuses on **fast execution** without hyperparameter tuning.  \n",
        "- Evaluated using key metrics such as **AUC-PR, Recall, Precision, F1-Score, and MCC**.  \n",
        "\n",
        "### **💡 Next Steps**  \n",
        "- Compare this **basic XGBoost model** with **Logistic Regression (Class Weights)** and **Random Forest (SMOTE Oversampling)**.  \n",
        "- Apply **hyperparameter tuning later** for better performance if needed.  \n"
      ],
      "metadata": {
        "id": "qI0F9kJbRxaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, classification_report\n",
        "\n",
        "# Compute scale_pos_weight for imbalance handling\n",
        "scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
        "\n",
        "# Train Basic XGBoost Model (GPU Accelerated)\n",
        "xgb_model = XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"aucpr\",\n",
        "    tree_method=\"hist\",\n",
        "    device=\"cuda\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Metrics\n",
        "xgb_metrics = {\n",
        "    \"AUC-PR\": average_precision_score(y_test, y_prob_xgb),\n",
        "    \"Recall\": recall_score(y_test, y_pred_xgb),\n",
        "    \"Precision\": precision_score(y_test, y_pred_xgb),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_xgb),\n",
        "    \"MCC\": matthews_corrcoef(y_test, y_pred_xgb)\n",
        "}\n",
        "\n",
        "# Display Metrics\n",
        "metrics_df_xgb = pd.DataFrame([xgb_metrics])\n",
        "\n",
        "# Print Summary\n",
        "print(\"\\n🔹 Basic XGBoost (Class Weights, GPU) Results:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LuG86f_dHv1l",
        "outputId": "1cb9e925-7449-4ec2-8ac6-4763b570c675"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:56:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Basic XGBoost (Class Weights, GPU) Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97      8880\n",
            "           1       0.12      0.34      0.17       163\n",
            "\n",
            "    accuracy                           0.94      9043\n",
            "   macro avg       0.55      0.65      0.57      9043\n",
            "weighted avg       0.97      0.94      0.96      9043\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:56:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Model 4: Basic LightGBM (Class Weights, CPU)**\n",
        "\n",
        "### **📌 Sampling Strategy:**  \n",
        "- **No SMOTE or undersampling is used**—instead, LightGBM handles class imbalance using **`scale_pos_weight`**.  \n",
        "- The **`scale_pos_weight`** is calculated as:  \n",
        "  \\[\n",
        "  \\frac{\\text{Number of Non-Defaulters}}{\\text{Number of Defaulters}}\n",
        "  \\]  \n",
        "  This ensures that the model **assigns higher weight** to the minority class (`default_encoded = 1`).  \n",
        "\n",
        "### **📌 Preprocessing & Model Training:**  \n",
        "- **No hyperparameter tuning** is applied to keep the model simple and efficient.  \n",
        "- A **basic LightGBM model** is trained with **CPU processing (`device=\"cpu\"`)** to ensure compatibility and stability.  \n",
        "\n",
        "### **📌 GPU Usage:**  \n",
        "- ❌ **No GPU is used** for this model.  \n",
        "- LightGBM runs on **CPU only** to avoid OpenCL issues and ensure smooth execution.  \n",
        "\n",
        "### **📌 Purpose:**  \n",
        "- Uses **class weights (`scale_pos_weight`) instead of data resampling**, keeping all data intact.  \n",
        "- Focuses on **fast execution** without hyperparameter tuning.  \n",
        "- Evaluated using key metrics such as **AUC-PR, Recall, Precision, F1-Score, and MCC**.  \n",
        "\n",
        "### **💡 Next Steps**  \n",
        "- Compare this **basic LightGBM model** with **Logistic Regression, Random Forest, and XGBoost**.  \n",
        "- Apply **hyperparameter tuning later** for better performance if needed.  \n"
      ],
      "metadata": {
        "id": "JxGoQBMIlgpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weight ratio\n",
        "scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
        "\n",
        "# Train Basic LightGBM Model (GPU Accelerated)\n",
        "lgbm_model = LGBMClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    boosting_type=\"gbdt\",\n",
        "    objective=\"binary\",\n",
        "    metric=\"average_precision\",\n",
        "    device=\"cpu\",  # ✅ Enable GPU acceleration\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_lgbm = lgbm_model.predict(X_test)\n",
        "y_prob_lgbm = lgbm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Metrics\n",
        "lgbm_metrics = {\n",
        "    \"AUC-PR\": average_precision_score(y_test, y_prob_lgbm),\n",
        "    \"Recall\": recall_score(y_test, y_pred_lgbm),\n",
        "    \"Precision\": precision_score(y_test, y_pred_lgbm),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_lgbm),\n",
        "    \"MCC\": matthews_corrcoef(y_test, y_pred_lgbm)\n",
        "}\n",
        "\n",
        "# Display Metrics\n",
        "metrics_df_lgbm = pd.DataFrame([lgbm_metrics])\n",
        "\n",
        "# Print Summary\n",
        "print(\"\\n🔹 Basic LightGBM (Class Weights, GPU) Results:\")\n",
        "print(classification_report(y_test, y_pred_lgbm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Of1T-kIvIa8V",
        "outputId": "e334d771-6851-4a39-c492-81bcf3676c87"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 652, number of negative: 35516\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011984 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 984\n",
            "[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018027 -> initscore=-3.997694\n",
            "[LightGBM] [Info] Start training from score -3.997694\n",
            "\n",
            "🔹 Basic LightGBM (Class Weights, GPU) Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94      8880\n",
            "           1       0.08      0.53      0.14       163\n",
            "\n",
            "    accuracy                           0.88      9043\n",
            "   macro avg       0.54      0.71      0.54      9043\n",
            "weighted avg       0.97      0.88      0.92      9043\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all model metrics into one DataFrame for easy comparison\n",
        "all_metrics = pd.DataFrame([\n",
        "    log_reg_metrics,  # Logistic Regression Metrics\n",
        "    rf_metrics,       # Random Forest Metrics\n",
        "    xgb_metrics,      # XGBoost Metrics\n",
        "    lgbm_metrics      # LightGBM Metrics\n",
        "])\n",
        "\n",
        "# Print the table for reference\n",
        "print(\"\\n🔹 Model Performance Comparison:\")\n",
        "print(all_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PwaqHpUgl5Pp",
        "outputId": "7e620df1-ad54-4b06-e59c-37dfdf90205d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Model Performance Comparison:\n",
            "     AUC-PR    Recall  Precision  F1-Score       MCC\n",
            "0  0.101233  0.852761   0.051424  0.096999  0.163919\n",
            "1  0.109390  0.202454   0.163366  0.180822  0.165130\n",
            "2  0.103068  0.337423   0.115546  0.172144  0.172783\n",
            "3  0.127667  0.533742   0.081157  0.140891  0.174021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Model Performance Comparison Summary**\n",
        "Below is the performance comparison of all four trained models using key evaluation metrics.\n",
        "\n",
        "| **Model**   | **AUC-PR** | **Recall** | **Precision** | **F1-Score** | **MCC**  |\n",
        "|------------|-----------|-----------|------------|-----------|-----------|\n",
        "| **Logistic Regression (Class Weights)** | 0.101233 | **0.852761** | 0.051424 | 0.096999 | 0.163919 |\n",
        "| **Random Forest (SMOTE Oversampling)** | 0.109390 | 0.202454 | 0.163366 | 0.180822 | 0.165130 |\n",
        "| **XGBoost (Class Weights, GPU)** | 0.103068 | 0.337423 | 0.115546 | 0.172144 | 0.172783 |\n",
        "| **LightGBM (Class Weights, GPU)** | **0.127667** | 0.533742 | **0.081157** | **0.140891** | **0.174021** |\n",
        "\n",
        "---\n",
        "\n",
        "### **📌 Key Observations**\n",
        "- **AUC-PR (Higher is better)** → **LightGBM (0.127667) performed the best**, indicating better ranking ability for positive (default) cases.\n",
        "- **Recall (Higher is better)** → **Logistic Regression (0.852761) has the highest recall**, meaning it captures most defaulters but with lower precision.\n",
        "- **Precision (Higher is better)** → **Random Forest (0.163366) has the best precision**, meaning it predicts fewer false positives.\n",
        "- **F1-Score (Higher is better)** → **Random Forest (0.180822) balances precision and recall well**.\n",
        "- **MCC (Higher is better)** → **LightGBM (0.174021) achieves the best MCC score**, indicating better overall classification balance.\n",
        "\n",
        "---\n",
        "\n",
        "### **✅ Final Verdict**\n",
        "✔ **LightGBM (Class Weights, GPU)** appears to be the **best overall model** based on **AUC-PR and MCC**.  \n",
        "✔ **Logistic Regression** is **too biased towards recall**, leading to many false positives.  \n",
        "✔ **Random Forest provides a balanced trade-off**, excelling in precision and F1-Score.  \n",
        "✔ **XGBoost performs moderately well** but does not outperform LightGBM.\n",
        "\n",
        "---\n",
        "\n",
        "### **🚀 Next Steps**\n",
        "- Fine-tune **LightGBM and Random Forest** to further improve **Precision and AUC-PR**.\n",
        "- Consider **feature selection** and **hyperparameter tuning** for better performance.\n",
        "- **Test ensemble models** combining **LightGBM and Random Forest** for optimal results.\n",
        "\n"
      ],
      "metadata": {
        "id": "lCNXBIzXOZYm"
      }
    }
  ]
}